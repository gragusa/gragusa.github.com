<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Codes on gragusa.org </title>
    <link>https://gragusa.org/code/index.xml</link>
    <language>en-us</language>
    <author>Giuseppe Ragusa</author>
    <rights>Copyright (c) Giuseppe Ragusa.</rights>
    <updated>Mon, 01 Jan 0001 00:00:00 UTC</updated>
    
    <item>
      <title>Resources</title>
      <link>https://gragusa.org/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
      <author>Giuseppe Ragusa</author>
      <guid>https://gragusa.org/code/</guid>
      <description>

&lt;p&gt;On this page I will keep a running list of computation/programming projects I work on from time to time.&lt;/p&gt;

&lt;p&gt;When it comes to coding, my interests range from estimation of nonlinear moment condition models, approximate bayesian inference, large optimization problems,
high-performance computing in econometrics and finance, and big data application to time time series econometrics.  Below you will find a list of package I have developed recently. For details on each packages visit my &lt;a href=&#34;http://github.com/gragusa&#34;&gt;github&lt;/a&gt; and/or visit the package page. For code related to publish paper, visit the publication section.&lt;/p&gt;

&lt;h1 id=&#34;julia-packages&#34;&gt;Julia Packages&lt;/h1&gt;

&lt;p&gt;Notice that some of the Julia packages are &amp;ldquo;registered&amp;rdquo;, meaning that you can install it from Julia by &lt;code&gt;Pkg.add&lt;/code&gt;-ing them. Others are at an early stage and are not yet registered.&lt;/p&gt;

&lt;h2 id=&#34;covariancematrices-jl-http-github-com-gragusa-covariancematrices-jl&#34;&gt;&lt;a href=&#34;http:://github.com/gragusa/CovarianceMatrices.jl&#34;&gt;&lt;code&gt;CovarianceMatrices.jl&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gragusa/CovarianceMatrices.jl/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg&#34; alt=&#34;license&#34; /&gt;&lt;/a&gt;  &lt;a href=&#34;http://pkg.julialang.org/?pkg=CovarianceMatrices&amp;amp;ver=0.5&#34;&gt;&lt;img src=&#34;http://pkg.julialang.org/badges/CovarianceMatrices_0.5.svg&#34; alt=&#34;CovarianceMatrices&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/gragusa/CovarianceMatrices.jl&#34;&gt;&lt;img src=&#34;https://travis-ci.org/gragusa/CovarianceMatrices.jl.svg?branch=master&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/gragusa/CovarianceMatrices.jl?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/gragusa/CovarianceMatrices.jl/badge.svg?branch=master&amp;amp;service=github&#34; alt=&#34;Coverage Status&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;CovarianceMatrices&lt;/code&gt; is a package for estimating variance covariance matrices in situations where the standard assumptions of independence is violated. It provides heteroskedasticity consistent (HC); heteroskedasticity and autocorrelation consistent (HAC); and cluster robust (CRVE) estimators of the variance matrices. An interface for &lt;code&gt;GLM.jl&lt;/code&gt; is given so that they can be integrated easily  into standard regression analysis. It is also easy to integrated these estimators into new inferential procedures or applications.&lt;/p&gt;



using CovarianceMatrices
## Simulated AR(1) and estimate it using OLS
srand(1)
y = zeros(Float64, 100)
rho = 0.8
y[1] = randn()
for j = 2:100
  y[j] = rho * y[j-1] + randn()
end

data = DataFrame(y = y[2:100], yl = y[1:99])
AR1  = fit(GeneralizedLinearModel, y~yl, data, Normal())

## Truncated Kernel with optimal bandwidth
vcov(AR1, TruncatedKernel())




&lt;h2 id=&#34;divergences-jl-http-github-com-gragusa-divergences-jl&#34;&gt;&lt;a href=&#34;http://github.com/gragusa/Divergences.jl&#34;&gt;&lt;code&gt;Divergences.jl&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gragusa/Divergences.jl/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg&#34; alt=&#34;license&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;http://pkg.julialang.org/detail/Divergences.html&#34;&gt;&lt;img src=&#34;http://pkg.julialang.org/badges/Divergences_0.5.svg&#34; alt=&#34;Pkg&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/gragusa/Divergences.jl&#34;&gt;&lt;img src=&#34;https://travis-ci.org/gragusa/Divergences.jl.svg?branch=master&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/gragusa/Divergences.jl?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/github/gragusa/Divergences.jl/badge.svg?branch=master&#34; alt=&#34;Coverage Status&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Divergences&lt;/code&gt; is a Julia package that makes it easy to evaluate divergence measures between two vectors. The package allows calculating the gradient and the diagonal of the Hessian of several divergences.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Komunjer, I.; Ragusa, G. &amp;ldquo;Existence and characterization of conditional density projections.&amp;rdquo; Econometric Theory 2016, 32, 947–987.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
A divergence between two vectors of probabilities; $p := (p\_1, p\_2,\ldots,p\_n)$ and $q := (q\_1, q\_2,\ldots,q\_n)$ is defined as
$$ D(p,q)= \sum\_{i=1}^{n} \phi \left(\frac{p\_{i}}{q\_{i}}\right)p\_{i} $$ where $\phi$ is function that satisfies the following:

1. is twice continuously differentiable on `$ (0, +\infty) $`;
2. is strictly convex on $(0, +\infty)$;
3. $\phi(1) = \phi&#39;(1) = 0$;
4. $\lim\_{u-&gt;0^+} \phi&#39;(u) &lt; 0$;
5. $\lim\_{u-&gt;+\infty} \phi&#39;(u) &gt; 0$.

An very general family of divergences is the Cressie-Read family[^1]. The class is indexed by a real parameter $\alpha$ and it is defined as
&lt;div&gt;
$$
\phi\_{\alpha}(u)=\begin{cases}
\frac{u^{\alpha+1}-1}{a(a+1)}-\frac{1}{a}u+\frac{1}{a} \&amp; u&gt;0 \\\\[2ex]
\frac{1}{a+1} \&amp; u=0
\end{cases}.
$$
&lt;/div&gt;

The package defines a `Divergence` type with the following suptypes:

* Kullback-Leibler divergence `KullbackLeibler`
* Chi-square distance `ChiSquared`
* Reverse Kullback-Leibler divergence `ReverseKullbackLeibler`
* Cressie-Read divergences `CressieRead`

These divergences differ from the equivalent ones defined in the `Distances` package because they are normalized. Also, the package provides methods for calculating their gradient and the (diagonal elements of the) Hessian matrix.

The constructors for the types above are straightforward
```julia
KullbackLeibler()
ChiSqaured()
ReverseKullbackLeibler()
```
The constructor for `CressieRead` is
```julia
CR(::Real)
```
The Hellinger divergence is obtained by `CR(-1/2)`. For a certain value of `alpha`, `CressieRead` correspond to a divergence that has a specific type defined. For instance `CR(1)` is equivalent to `ChiSquared` although the underlying code for evaluation and calculation of the gradient and Hessian are different. --&gt;



using Divergences
p = rand(20)
q = rand(20)
scale!(p, 1/sum(p))
scale!(q, 1/sum(q))
evaluate(CressieRead(-.5), p, q)



&lt;h2 id=&#34;genoud-jl-http-github-org-gragusa-genoud-jl&#34;&gt;&lt;a href=&#34;http://github.org/gragusa/Genoud.jl&#34;&gt;&lt;code&gt;Genoud.jl&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gragusa/Genoud.jl/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-GPL3.0+-blue.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;code/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Julia-unregistered-red.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GENetic Optimization Using Derivative.&lt;/p&gt;

&lt;div class=&#34;units-row&#34;&gt;

&lt;div class=&#34;unit-65&#34;&gt;




using Genoud
using Calculus
function f8(xx)
    x, y = xx
    -x*sin(√abs(x)) - y*sin(√abs(y))
end

function gr!(x, stor)  
    stor[:] = Calculus.gradient(f8, x)
end

dom = Genoud.Domain([-500  500.;
                     -500. 500.])
out = Genoud.genoud(f8, [1.0, -1.0],
                    sizepop = 5000,
                    sense = :Min,
                    domains = dom)



&lt;/div&gt;

&lt;div class=&#34;unit-35&#34;&gt;

&lt;figure&gt;
    &lt;img src=&#34;fig/f8.png&#34;  /&gt;
    &lt;figcaption&gt;
        &lt;h4&gt;Surface plot of $$f(x_1, x_2) = -\sum_{i=1}^2 x_i \sin(\sqrt{|x_i|}).$$ This function is minimized at $x_1^*  \approx 420.968$ and $x_2^* \approx 420.968$. At the minima, $f(x_1^*, x_2^*) = -837.9$.
        &lt;p&gt;&lt;/p&gt;
        Source: Yao, Xin, Yong Liu, and Guangming Lin. &#34;Evolutionary programming made faster.&#34; IEEE Transactions on Evolutionary computation 3, no. 2 (1999): 82-102.&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;/div&gt;
&lt;/div&gt;



Results of Genoud Optimization Algorithm
 * Minimizer: [420.96874636091724,420.9687462145861]
 * Minimum: -8.379658e+02
 * Pick generation: 20
 * Convergence: true
   * |f(x) - f(x&#39;)| / |f(x)| &lt; 1.0e-03: true
   * Number of Generations: 27



&lt;h2 id=&#34;csminwel-jl-http-github-org-gragusa-csminwel-jl&#34;&gt;&lt;a href=&#34;http://github.org/gragusa/CsminWel.jl&#34;&gt;&lt;code&gt;CsminWel.jl&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gragusa/CsminWel.jl/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD3-blue.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;code/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Julia-unregistered-red.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;code/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Julia-unregistered-red.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/gragusa/CsminWel.jl&#34;&gt;&lt;img src=&#34;https://travis-ci.org/gragusa/CsminWel.jl.svg?branch=master&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/gragusa/CsminWel.jl?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/gragusa/CsminWel.jl/badge.svg?branch=master&amp;amp;service=github&#34; alt=&#34;Coverage Status&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/gragusa/CsminWel.jl?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/gragusa/CsminWel.jl/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This package provides an interface to Chris Sims&amp;rsquo; &lt;code&gt;csminwel&lt;/code&gt; optimization code. The code borrows from &lt;a href=&#34;https://github.com/FRBNY-DSGE/DSGE.jl&#34;&gt;DSGE.jl&lt;/a&gt;, but it is adapted to be compatibles with the &lt;a href=&#34;https://github.com/JuliaOpt/Optim.jl&#34;&gt;Optim.jl&lt;/a&gt;&amp;rsquo;s API. When the derivative of the minimand is not supply either Finite Difference of Forward Automatic Differentiation derivatives are automatically supplied to the underlying code.&lt;/p&gt;

&lt;p&gt;From the original author:
&amp;gt; Uses a quasi-Newton method with BFGS update of the estimated inverse hessian. It is robust against certain pathologies common on likelihood functions. It attempts to be robust against &amp;ldquo;cliffs&amp;rdquo;, i.e. hyperplane discontinuities, though it is not really clear whether what it does in such cases succeeds reliably.&lt;/p&gt;

&lt;p&gt;Differently from the solvers in &lt;code&gt;Optim.jl&lt;/code&gt;, &lt;code&gt;Csminwel&lt;/code&gt; returns an estimate of the inverse of the Hessian at the solution which may be used for standard errors calculations and/or to scale a Monte Carlo sampler.&lt;/p&gt;



#=
Maximizing loglikelihood of logistic models
=#
using CsminWel
using StatsFuns
## Generate fake data (true coefficient = 0)
srand(1)
x = [ones(200) randn(200,4)]
y = [rand() &lt; 0.5 ? 1. : 0. for j in 1:200]

## log-likelihood
function loglik(beta)
    xb = x*beta
    sum(-y.*xb + log1pexp.(xb))
end

## Derivative of loglikelihood
function dloglik(beta)
    xb = x*beta
    px = logistic.(xb)
    -x&#39;*(y.-px)
end

## Optim uses a mutating function for deriv
function fg!(beta, stor)
    stor[:] = dloglik(beta)
end

## With analytical derivative
res1 = optimize(loglik, fg!, zeros(5), BFGS())
res2 = optimize(loglik, fg!, zeros(5), Csminwel())

## With finite-difference derivative
res3 = optimize(loglik, zeros(5), Csminwel())

## With forward AD derivative
res4 = optimize(loglik, zeros(5), Csminwel(), OptimizationOptions(autodiff=true))

## Use approximation to the inverse Hessian for standard errors of estimated parameters
stderr = √diag(res2.invH)


</description>
    </item>
    
  </channel>
</rss>
